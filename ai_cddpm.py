# -*- coding: utf-8 -*-
"""cddpm ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-eM32bwQIZ3z2v3voNo8IkOJSC6eCRHE
"""


import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import math
import logging
from torchvision.utils import save_image
import matplotlib.pyplot as plt

# Setup logging
logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
start = 21
# ----------------------
# Dataset
# ----------------------
from torchvision import datasets, transforms
im_size = 96.

transform = transforms.Compose([
        transforms.Resize((im_size, im_size)),
        transforms.ToTensor()
    ])

# load dataset
PATH = '/opt/data/reyhanian/data/affectnet'

dataset = datasets.ImageFolder(f'{PATH}/train', transform=transform)
val_dataset = datasets.ImageFolder(f'{PATH}/valid', transform=transform)

dataset[0][0].max()

# ----------------------
# UNet Components with Attention
# ----------------------
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, emb_channels):
        super().__init__()
        self.norm1 = nn.GroupNorm(1, in_channels)
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.norm2 = nn.GroupNorm(1, out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.emb_proj = nn.Linear(emb_channels, out_channels)
        self.skip = nn.Conv2d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()

    def forward(self, x, t_emb):
        h = F.relu(self.norm1(x))
        h = self.conv1(h)
        h = h + self.emb_proj(t_emb)[:, :, None, None]
        h = F.relu(self.norm2(h))
        h = self.conv2(h)
        return h + self.skip(x)


class SelfAttention(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.q = nn.Conv1d(in_channels, in_channels // 8, 1)
        self.k = nn.Conv1d(in_channels, in_channels // 8, 1)
        self.v = nn.Conv1d(in_channels, in_channels, 1)
        self.gamma = nn.Parameter(torch.zeros(1))

    def forward(self, x):
        B, C, H, W = x.size()
        x_flat = x.view(B, C, -1)
        q = self.q(x_flat).permute(0, 2, 1)
        k = self.k(x_flat)
        v = self.v(x_flat)

        attn = torch.bmm(q, k)
        attn = F.softmax(attn, dim=-1)

        out = torch.bmm(v, attn.permute(0, 2, 1))
        out = out.view(B, C, H, W)

        return self.gamma * out + x


class Downsample(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv = nn.Conv2d(channels, channels, 4, 2, 1)

    def forward(self, x):
        return self.conv(x)


class Upsample(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv = nn.ConvTranspose2d(channels, channels, 4, 2, 1)

    def forward(self, x):
        return self.conv(x)


class DDPMUNet(nn.Module):
    def __init__(self, num_classes, in_channels=3, base_channels=128):
        super().__init__()
        self.num_classes = num_classes
        self.label_emb = nn.Embedding(num_classes * num_classes, 128)

        self.time_mlp = nn.Sequential(
            nn.Linear(1, 128), nn.ReLU(),
            nn.Linear(128, 128)
        )

        self.enc1 = ResidualBlock(in_channels, base_channels, 128)
        self.attn1 = SelfAttention(base_channels)
        self.down1 = Downsample(base_channels)
        self.enc2 = ResidualBlock(base_channels, base_channels*2, 128)
        self.attn2 = SelfAttention(base_channels*2)
        self.down2 = Downsample(base_channels*2)
        self.enc3 = ResidualBlock(base_channels*2, base_channels*4, 128)

        self.dec3 = ResidualBlock(base_channels*4, base_channels*2, 128)
        self.up2 = Upsample(base_channels*2)
        self.dec2 = ResidualBlock(base_channels*2, base_channels, 128)
        self.up1 = Upsample(base_channels)
        self.dec1 = ResidualBlock(base_channels, base_channels, 128)

        self.final = nn.Conv2d(base_channels, in_channels, 1)

    def forward(self, x, y_source, y_target, t):
        y_source = y_source.long()
        y_target = y_target.long()
        t = t.view(-1, 1).float() / 1000
        t_emb = self.time_mlp(t)

        index = y_source + y_target * self.num_classes
        index = index.clamp(0, self.label_emb.num_embeddings - 1)
        emb = self.label_emb(index) + t_emb

        h1 = self.enc1(x, emb)
        h1 = self.attn1(h1)
        h2 = self.enc2(self.down1(h1), emb)
        h2 = self.attn2(h2)
        h3 = self.enc3(self.down2(h2), emb)

        d3 = self.dec3(h3, emb)
        d2 = self.dec2(self.up2(d3) + h2, emb)
        d1 = self.dec1(self.up1(d2) + h1, emb)

        return self.final(d1)

class DDPMPipeline:
    def __init__(self, model, num_timesteps=1000, device='cuda'):
        self.model = model.to(device)
        self.device = device
        self.num_timesteps = num_timesteps
        self.betas = cosine_beta_schedule(num_timesteps).to(device)
        self.alphas = 1. - self.betas
        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)
        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0], device=device), self.alphas_cumprod[:-1]])

    @torch.no_grad()
    def reenact(self, image, source_label, target_label):
        self.model.eval()
        x = image.unsqueeze(0).to(self.device)
        y_source = torch.tensor([source_label], device=self.device).long()
        y_target = torch.tensor([target_label], device=self.device).long()

        t_noise = torch.randint(0, self.num_timesteps, (1,), device=self.device)
        noise = torch.randn_like(x)
        alpha_t = self.alphas_cumprod[t_noise].view(-1, 1, 1, 1)
        x = torch.sqrt(alpha_t) * x + torch.sqrt(1 - alpha_t) * noise

        for t in reversed(range(self.num_timesteps)):
            t_tensor = torch.full((1,), t, device=self.device, dtype=torch.long)
            predicted_noise = self.model(x, y_source, y_target, t_tensor)

            alpha_t = self.alphas[t]
            alpha_bar_t = self.alphas_cumprod[t]
            alpha_bar_prev = self.alphas_cumprod_prev[t]

            coef1 = 1 / torch.sqrt(alpha_t)
            coef2 = (1 - alpha_t) / torch.sqrt(1 - alpha_bar_t)
            pred_x0 = (x - coef2 * predicted_noise) / coef1
            mean = torch.sqrt(alpha_bar_prev) * pred_x0 + torch.sqrt(1 - alpha_bar_prev) * predicted_noise

            if t > 0:
                noise = torch.randn_like(x)
                x = mean + torch.sqrt(self.betas[t]) * noise
            else:
                x = mean

        return x.clamp(0, 1)

# ----------------------
# Training Utility
# ----------------------
class DiffusionTrainer:
    def __init__(self, pipeline, dataloader, optimizer, scheduler=None, epochs=200, val_loader=None):
        self.pipeline = pipeline
        self.model = pipeline.model
        self.dataloader = dataloader
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.epochs = epochs
        self.device = pipeline.device
        self.val_loader = val_loader

    def train(self):
        self.model.train()
        best_loss = float('inf')
        train_losses = []

        for epoch in range(start, self.epochs):
            epoch_loss = 0
            loop = tqdm(self.dataloader, desc=f"Epoch [{epoch+1}/{self.epochs}]", leave=False)
            for images, labels in loop:
                images, labels = images.to(self.device), labels.to(self.device)
                t = torch.randint(0, self.pipeline.num_timesteps, (images.size(0),), device=self.device).long()
                noise = torch.randn_like(images)
                alpha_t = self.pipeline.alphas_cumprod[t].view(-1, 1, 1, 1)
                noisy_images = torch.sqrt(alpha_t) * images + torch.sqrt(1 - alpha_t) * noise

                y_target = torch.randint(0, self.model.num_classes, labels.shape, device=self.device)
                predicted_noise = self.model(noisy_images, labels, y_target, t)
                loss = F.mse_loss(predicted_noise, noise)
                train_losses.append(loss)

                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()
                loop.set_postfix(loss=loss.item())
                epoch_loss += loss.item()

            avg_loss = epoch_loss / len(self.dataloader)
            logging.info(f"Epoch {epoch+1}: Loss = {avg_loss:.6f}")

            if self.scheduler:
                self.scheduler.step()

            if avg_loss < best_loss or epoch%5 == 0:
                best_loss = avg_loss
                torch.save(self.model.state_dict(), f"/opt/data/reyhanian/ai_checkpoints/best_model_epoch{epoch+1}.pt")
                logging.info(f"Saved checkpoint at epoch {epoch+1}")

                #save image
                image, label = dataset[0]
                reenacted = self.pipeline.reenact(image.to(device), source_label=label, target_label=3)
                show_image(reenacted, epoch+1)

            if self.val_loader:
                self.validate()

        return train_losses

    def validate(self):
        self.model.eval()
        val_loss = 0
        with torch.no_grad():
            for images, labels in self.val_loader:
                images, labels = images.to(self.device), labels.to(self.device)
                t = torch.randint(0, self.pipeline.num_timesteps, (images.size(0),), device=self.device).long()
                noise = torch.randn_like(images)
                alpha_t = self.pipeline.alphas_cumprod[t].view(-1, 1, 1, 1)
                noisy_images = torch.sqrt(alpha_t) * images + torch.sqrt(1 - alpha_t) * noise

                y_target = torch.randint(0, self.model.num_classes, labels.shape, device=self.device)
                predicted_noise = self.model(noisy_images, labels, y_target, t)
                loss = F.mse_loss(predicted_noise, noise)
                val_loss += loss.item()

        avg_val_loss = val_loss / len(self.val_loader)
        logging.info(f"Validation Loss: {avg_val_loss:.6f}")
        self.model.train()

# ----------------------
# Utils
# ----------------------
def cosine_beta_schedule(timesteps, s=0.008):
    steps = timesteps + 1
    x = torch.linspace(0, timesteps, steps)
    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clip(betas, 0.0001, 0.9999)


def show_image(tensor, i):
    img = tensor.squeeze().detach().cpu()
    img = torch.clamp(img, 0.0, 1.0)
    plt.imshow(img.permute(1, 2, 0))
    plt.axis("off")
    plt.savefig(f'fake_images/{i}_img0.png')


# ----------------------
# Debug helper
# ----------------------

# ----------------------
# Main entry point
# ----------------------
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

def plot_losses(train_losses, val_losses=None):
    plt.figure(figsize=(10, 4))
    plt.plot(train_losses, label='Train Loss')
    if val_losses:
        plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss over Epochs')
    plt.legend()
    plt.grid(True)
    plt.savefig("loss_plot.png")
    plt.show()

if __name__ == '__main__':
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)

    PATH = f"/opt/data/reyhanian/ai_checkpoints/best_model_epoch{start}.pt"
    # model = DDPMUNet(num_classes=7).to(device)
    model = DDPMUNet(num_classes=7).cuda()
    model.load_state_dict(torch.load(PATH), strict=False)

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)

    pipeline = DDPMPipeline(model, device=device)
    trainer = DiffusionTrainer(pipeline, dataloader, optimizer, scheduler, epochs=100, val_loader=val_loader)
    train_losses = trainer.train()

    image, label = dataset[0]
    reenacted = pipeline.reenact(image.to(device), source_label=label, target_label=3)
    show_image(reenacted, 00)
    plot_losses(train_losses)



